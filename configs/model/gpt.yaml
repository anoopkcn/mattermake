_target_: src.models.gpt_module.GPTModule

# Model architecture
n_layer: 48
n_head: 25
n_embd: 1600
block_size: 1024
bias: false
vocab_size: 94  # Match your vocabulary size
dropout: 0.0
embedding_dim: 256

# Optimizer settings
learning_rate: 3e-4
weight_decay: 1e-1
betas: [0.9, 0.95]

# Learning rate scheduler
warmup_iters: 150
lr_decay_iters: 600000
min_lr: 3e-5
decay_lr: true
