_target_: models.cross_attention_gpt_module.CrossAttentionGPTModule

# Model architecture
n_layer: 12
n_head: 12
n_embd: 768
block_size: 1024
bias: false
vocab_size: 94  # Match your vocabulary size
dropout: 0.0
embedding_dim: 256

# Optimizer settings
learning_rate: 3e-4
weight_decay: 1e-1
betas: [0.9, 0.95]

# Learning rate scheduler
warmup_iters: 150
lr_decay_iters: 600000
min_lr: 3e-5
decay_lr: true
